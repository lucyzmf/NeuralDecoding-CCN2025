{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"id":"Xd_1kxqWVQ_w","cellView":"form","collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754926275595,"user_tz":-120,"elapsed":33263,"user":{"displayName":"ccn2025_brainai","userId":"06076816389518067620"}},"outputId":"2a02d4b8-3519-4b38-b3ee-744834f8a75b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: mne in /usr/local/lib/python3.11/dist-packages (1.10.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne) (4.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne) (3.1.6)\n","Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne) (0.4)\n","Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.11/dist-packages (from mne) (3.10.0)\n","Requirement already satisfied: numpy<3,>=1.25 in /usr/local/lib/python3.11/dist-packages (from mne) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne) (25.0)\n","Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne) (1.8.2)\n","Requirement already satisfied: scipy>=1.11 in /usr/local/lib/python3.11/dist-packages (from mne) (1.16.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne) (4.67.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->mne) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->mne) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->mne) (4.59.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->mne) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->mne) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->mne) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->mne) (2.9.0.post0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (4.3.8)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (2.32.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne) (3.0.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7->mne) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.8.3)\n","Requirement already satisfied: wordfreq in /usr/local/lib/python3.11/dist-packages (3.1.1)\n","Requirement already satisfied: ftfy>=6.1 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (6.3.1)\n","Requirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (3.5.0)\n","Requirement already satisfied: locate<2.0.0,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (1.1.1)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (1.1.1)\n","Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (2024.11.6)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes>=3.0->wordfreq) (1.3.0)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes>=3.0->wordfreq) (1.2.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from marisa-trie>=1.1.0->language-data>=1.2->langcodes>=3.0->wordfreq) (75.2.0)\n","Requirement already satisfied: exca in /usr/local/lib/python3.11/dist-packages (0.4.6)\n","Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.11/dist-packages (from exca) (2.0.2)\n","Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.11/dist-packages (from exca) (6.0.2)\n","Requirement already satisfied: pydantic>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from exca) (2.11.7)\n","Requirement already satisfied: submitit>=1.5.1 in /usr/local/lib/python3.11/dist-packages (from exca) (1.5.3)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.0->exca) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.0->exca) (2.33.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.0->exca) (4.14.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.0->exca) (0.4.1)\n","Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from submitit>=1.5.1->exca) (3.1.1)\n"]},{"output_type":"execute_result","data":{"text/plain":["[PosixPath('data/libribrain2025_data.zip'),\n"," PosixPath('data/libribrain2025_data'),\n"," PosixPath('data/utils.py'),\n"," PosixPath('data/images')]"]},"metadata":{},"execution_count":6}],"source":["#@title Prep\n","!pip install mne\n","!pip install wordfreq\n","!pip install exca\n","\n","from pathlib import Path\n","if not Path(\"data\").exists():\n","  !gdown --folder --id 1DjONQkI2m2_cJ6k-bc4ahgmCoFlCi97w -O data\n","\n","if not Path(\"data/libribrain2025_data\").exists():\n","  !unzip -q data/libribrain2025_data.zip -d data/libribrain2025_data\n","\n","[i for i in Path(\"data\").glob(\"*\")]"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"hUEL0dPAStx1","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":271,"output_embedded_package_id":"1VjWoQuubiD8WMStukClNX6qtdeoxempJ"},"executionInfo":{"status":"ok","timestamp":1754926344880,"user_tz":-120,"elapsed":1183,"user":{"displayName":"ccn2025_brainai","userId":"06076816389518067620"}},"outputId":"67f5b823-9b4f-4101-8b17-ee46bbd47ec2"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["#@title Intro\n","from IPython.display import display, HTML\n","from base64 import b64encode\n","\n","# Main profile image path\n","main_path = \"/content/data/images/LucyZhang.jpg\"\n","\n","# Two small logos paths\n","logo1_path = \"/content/data/images/Meta_lockup_positive primary_RGB.png\" # Replace with actual paths\n","logo2_path = \"/content/data/images/rothschild_logo.png\"\n","logo3_path = \"/content/data/images/ENS_PSL.jpg\"\n","\n","# Encode images in base64\n","def encode_img(path):\n","    with open(path, \"rb\") as f:\n","        return b64encode(f.read()).decode(\"utf-8\")\n","\n","main_img = encode_img(main_path)\n","logo1_img = encode_img(logo1_path)\n","logo2_img = encode_img(logo2_path)\n","logo3_img = encode_img(logo3_path)\n","\n","html = f\"\"\"\n","<div style=\"display: flex; align-items: flex-start; justify-content: space-between;\">\n","    <!-- Left side: profile image and text -->\n","    <div style=\"display: flex; align-items: flex-start;\">\n","        <img src=\"data:image/png;base64,{main_img}\" width=\"200\" style=\"margin-right: 20px;\"/>\n","        <div style=\"font-size: 24px; font-weight: bold; margin-bottom: 8px;\">\n","            <b>Part 2: Developing efficiently: cluster and caching</b><br>\n","            <div style=\"font-size: 18px; line-height: 1.5; font-weight: normal;\">\n","                Lucy (Mingfang) Zhang<br>\n","                PhD at École normale supérieure - PSL, Paris & Rothschild Foundation Hospital. <br><br>\n","                Relevant <a href=\"https://arxiv.org/abs/2502.07429\" target=\"_blank\">paper</a>: <br>\n","                \"From Thought to Action: How a Hierarchy of Neural Dynamics Supports Language Production\"<br>\n","                Decoding of linguistic features during continuous natrual language production. <br><br>\n","                Keep in touch:\n","                               <a href=\"https://www.linkedin.com/in/lucy-mingfang-zhang/\" target=\"_blank\">Linkedin</a>,\n","                               <a href=\"https://scholar.google.com/citations?view_op=list_works&hl=en&user=23vdTiQAAAAJ\" target=\"_blank\">Scholar</a>,\n","            </div>\n","        </div>\n","    </div>\n","\n","    <!-- Right side: two small logos -->\n","    <div style=\"display: flex; flex-direction: column; gap: 10px; margin-left: 20px;\">\n","    <div style=\"display: flex; flex-direction: column; gap: 10px; margin-left: 20px; align-items: center;\">\n","\n","        <img src=\"data:image/png;base64,{logo1_img}\" width=\"200\" style=\"object-fit: contain;\" />\n","        <img src=\"data:image/png;base64,{logo2_img}\" width=\"100\" style=\"object-fit: contain;\" />\n","        <img src=\"data:image/png;base64,{logo3_img}\" width=\"100\" style=\"object-fit: contain;\" />\n","\n","    </div>\n","</div>\n","\"\"\"\n","\n","display(HTML(html))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NmitLXeJStx2"},"outputs":[],"source":["import pydantic\n","import spacy\n","from pathlib import Path\n","import mne\n","import numpy as np\n","import typing as tp\n","from scipy.stats import pearsonr\n","from sklearn.linear_model import Ridge\n","from sklearn.model_selection import KFold, cross_val_predict\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","from tqdm import trange\n","from wordfreq import zipf_frequency\n","\n","import spacy.cli\n","spacy.cli.download(\"en_core_web_lg\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vsPX9FjLStx3"},"outputs":[],"source":["import pandas as pd\n","from exca import TaskInfra\n","import time\n","import copy"]},{"cell_type":"markdown","metadata":{"id":"LbJy1X9RStx3"},"source":["# Exca ⚔ - Execution and caching\n","\n","Documentation: https://facebookresearch.github.io/exca/index.html\n","\n","### Key takeaways:\n","Exca is a handy tool that allows you to\n","- modular pipeline with simple building blocks\n","- cached results so that only missing elements of the array get sent and avoid recomputation\n","- easy remote computation configuration\n","- validated configuration before sending to remote cluster through a job array\n"]},{"cell_type":"markdown","metadata":{"id":"iH55PvvoStx4"},"source":["# The Goal of this part:\n","\n","Set up an experimental pipeline that goes from yaml -> results (cache)\n","\n","#### What are the benefits?\n","- easy to change parameters\n","- modular, easy to continuously maintain as experiment continues to get more complex\n","- cache preprocessed data and experiment results, avoid recomputation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8S3g9SEwStx4"},"outputs":[],"source":["DATA_DIR = Path(\"/content/data/libribrain2025_data\") /\"checkpoint\"/\"hubertjb\"/\"ccn2025\"/\"data\"/\"libribrain2025\"\n","CACHE_DIR = \"/content/data/cache/libribrain2025\""]},{"cell_type":"markdown","metadata":{"id":"7zTbuejfStx4"},"source":["With these classes we've been seen before and add exca (later in this session)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQI_XMcMStx4"},"outputs":[],"source":["# Neuro class that handles the neural data\n","class Neuro(pydantic.BaseModel):\n","\n","    preproc_path: Path\n","    fmin: float = 0.05\n","    fmax: float = 40.\n","    freq: float = 80.\n","    tmin: float = -.5\n","    tmax: float = 1.\n","\n","    model_config = pydantic.ConfigDict(extra=\"forbid\")  # forbit model to accept extra attributes\n","\n","    def prepare_neuro(self, session: str) -> mne.io.Raw:\n","\n","        \"\"\" Load the raw neuro data and filter \"\"\"\n","\n","        file= Path(self.preproc_path) / f\"{session}_preproc.fif\"\n","\n","        if file.exists():\n","            raw = mne.io.read_raw(file, verbose=\"ERROR\")\n","\n","        else:\n","            fmin = self.fmin\n","            fmax = self.fmax\n","            freq = self.freq\n","\n","            original_file = self.preproc_path / f\"{session}.fif\" # original file\n","            raw = mne.io.read_raw(original_file)\n","            raw = raw.pick(picks=[\"meg\"]) # don't want to analyse misc\n","\n","            # band pass filter\n","            raw = raw.filter(fmin, fmax)\n","\n","            # downsample\n","            if freq != raw.info[\"sfreq\"]:\n","                raw = raw.resample(freq)\n","\n","        return raw\n","\n","    def __call__(self, session:str) -> tuple[np.ndarray, list[str]]:\n","\n","        \"\"\" Segment the neural data around words \"\"\"\n","\n","        raw = self.prepare_neuro(session)\n","\n","        events = pd.read_csv(self.preproc_path / \"events.csv\")\n","\n","        # Select the words in the relevant session\n","        words = events[(events['type'] == 'Word') & (events['session'] == session)].dropna().reset_index(drop=True)\n","\n","        # Get word onsets in samples\n","        word_onsets = np.ones((len(words), 3), dtype=int) # mne.epochs expects events of shape (n_events, 3) but we are only interested in the first column here -> set the rest to 0\n","        word_onsets[:, 0] = words.start *raw.info[\"sfreq\"] # first column must contain the onset of each event (word) in samples\n","\n","        # Segment\n","        segments = mne.Epochs(\n","            raw,\n","            word_onsets,\n","            metadata=words,\n","            event_repeated=\"drop\",\n","            baseline=(-0.2, 0),  # setting a baseline (-0.2, 0) can improve decoding results. Baselining subtracts the mean value over this window from the entire segment.\n","            tmin=self.tmin,\n","            tmax=self.tmax,\n","            verbose=\"ERROR\"\n","        )\n","\n","        # from mne to numpy\n","        neuro_array = segments.get_data(verbose=\"ERROR\")\n","\n","        # clip segments to prevent outliers impacting regression\n","        neuro_array = np.clip(neuro_array, a_min=-20, a_max=20)\n","\n","        n_words, n_channels, n_times = neuro_array.shape\n","        words = segments.metadata['text']\n","\n","        return neuro_array, words\n","\n","# Feature class that computes word frequency or word embedding\n","class Feature(pydantic.BaseModel):\n","    feature: tp.Literal['zipf_frequency', 'word_embedding']\n","    _model = None\n","\n","    model_config = pydantic.ConfigDict(extra=\"forbid\")  # forbit model to accept extra attributes\n","\n","    def __call__(self, word: str) -> list[float]:\n","\n","        \" Compute word frequency or word embedding for a single word \"\n","\n","        if self.feature == 'zipf_frequency':\n","            return [zipf_frequency(word, 'en')]\n","        else:\n","            if self._model is None:\n","                self._model = spacy.load(\"en_core_web_sm\")\n","            word_embedding = self._model(word).vector\n","            return word_embedding.tolist()\n","\n","# Data class that combines neural data and feature extraction\n","class Data(pydantic.BaseModel):\n","    neuro: Neuro\n","    feature: Feature\n","    n_sessions: int = 1\n","\n","    model_config = pydantic.ConfigDict(extra=\"forbid\")  # forbit model to accept extra attributes\n","\n","    def __call__(self) -> tuple[np.ndarray, np.ndarray]:\n","\n","        \"\"\" concatenate neural data and feature over multiple sessions \"\"\"\n","\n","        print(\"Preparing data\")\n","\n","        # get data\n","        neuro_array = []\n","        words = []\n","\n","        sessions = [i.name.split(\"_preproc\")[0] for i in sorted(list(self.neuro.preproc_path.glob(\"*.fif\")))] # neuro files available in path\n","        if self.n_sessions > len(sessions):\n","            raise ValueError(f\"You requested {self.n_sessions} but there are only {len(sessions)} available.\")\n","\n","        for session in sessions[:self.n_sessions]:\n","            session_neuro_array, session_words = self.neuro(session)\n","            neuro_array.append(session_neuro_array)\n","            words.extend(session_words)\n","        neuro_array = np.concatenate(neuro_array, 0)\n","\n","        # compute embedding\n","        feature_array = np.asarray([self.feature(word) for word in words])\n","        n_words, n_dims = feature_array.shape\n","        return neuro_array, feature_array\n","\n","# Experiment class that runs the decoding experiment\n","class Experiment(pydantic.BaseModel):\n","    data: Data\n","    n_folds: int = 5\n","    infra: TaskInfra = TaskInfra() # infra function for caching and and remote computation\n","\n","    model_config = pydantic.ConfigDict(extra=\"forbid\")  # forbit model to accept extra attributes\n","\n","    @infra.apply\n","    def run(self) -> np.ndarray:\n","\n","        \"\"\" fit a linear decoding model \"\"\"\n","\n","        print(f\"Decoding {self.data.feature.feature} from {self.data.n_sessions} sessions\")\n","\n","        neuro_array, feature_array = self.data()\n","        n_words, n_channels, n_times = neuro_array.shape\n","        n_words, n_dims = feature_array.shape\n","\n","        print(\"Fitting model\")\n","        # init model\n","        model = make_pipeline(StandardScaler(), Ridge(alpha=1e4))\n","\n","        # init cross validation\n","        cv = KFold(self.n_folds, shuffle=False)\n","\n","        # experiment loop\n","        r_scores = np.zeros((cv.n_splits, n_times, n_dims))\n","        for split, (train, test) in enumerate(cv.split(neuro_array)):\n","            for t in trange(n_times, desc=f\"split {split}/{cv.n_splits}\"):\n","                model.fit(X=neuro_array[train, :, t], y=feature_array[train])\n","                preds = model.predict(X=neuro_array[test, :, t])\n","                if len(preds.shape) == 1:\n","                  preds = preds[:, np.newaxis]\n","\n","                for d in range(n_dims):\n","                    r_scores[split, t, d] = pearsonr(\n","                        preds[:, d],\n","                        feature_array[test, d]\n","                    ).statistic\n","        # mean across splits\n","        r = r_scores.mean(0)\n","        # mean across feature dimensions\n","        r = r.mean(1)\n","        return r\n"]},{"cell_type":"markdown","metadata":{"id":"9J5XmvqmStx5"},"source":["We can do..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oe4zLf-bStx5"},"outputs":[],"source":["import yaml\n","\n","config = f\"\"\"\n","data:\n","    neuro:\n","        preproc_path: {DATA_DIR}\n","        fmin: 0.05\n","        fmax: 40.\n","        freq: 80.\n","    feature:\n","        feature: zipf_frequency\n","    n_sessions: 1\n","n_folds: 5\n","infra:\n","    folder: {CACHE_DIR}\n","\"\"\"\n","\n","config = yaml.safe_load(config)\n","exp = Experiment(**config)\n","\n","# Run the experiment, and time\n","start_time = time.time()\n","r = exp.run()\n","end_time = time.time()\n","\n","print(\"Decoding results shape:\", r.shape)\n","print(f\"Run time: {end_time - start_time}\")\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rN75m0WEStx5"},"outputs":[],"source":["# when we rerun you'll see that no recomputation is done, as the results are cached\n","start_time = time.time()\n","r = exp.run()\n","end_time = time.time()\n","\n","print(\"Decoding results shape:\", r.shape)\n","print(f\"Run time: {end_time - start_time}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"csTOQsX0Stx6"},"outputs":[],"source":["#@title Plot results\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","def plot_results(times, r, feature_name=\"zipf_frequency\"):\n","    results = pd.DataFrame(dict(times=times, r=r))\n","    fig, ax = plt.subplots()\n","    sns.lineplot(results, x=\"times\", y=\"r\", ax=ax)\n","    ax.axhline(0, color=\"k\", ls=\"--\", zorder=-5)\n","    ax.axvline(0, color=\"grey\", ls=\"--\", zorder=-5, label=\"Event onset\")\n","    ax.set_title(f\"Decoding {feature_name} over time\")\n","    ax.set_ylabel(\"Correlation\")\n","    ax.set_xlabel(\"Times relative to word onset (s)\")\n","    plt.legend()\n","    plt.show()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yI-Ptnm-YjuS"},"outputs":[],"source":["times = np.linspace(exp.data.neuro.tmin, exp.data.neuro.tmax, r.shape[0])\n","\n","plot_results(times, r, feature_name=exp.data.feature.feature)"]},{"cell_type":"markdown","metadata":{"id":"mGgm7c4TStx6"},"source":["# Pydantic and Exca\n","\n","We use two main tools to build this pipeline.\n","- Pydantic: helps you check that your data is the right type and format (parameter validation). You just describe what your data should look like, and Pydantic makes sure it matches, giving clear errors if something’s wrong\n","- Exca: excute computations locally or remotely and cache the results"]},{"cell_type":"markdown","metadata":{"id":"YYqDj1r4Stx6"},"source":["## Class construction and parameter validation in pydantic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DUeXd2mNStx6"},"outputs":[],"source":["# normal python class\n","class MyTask:\n","    def __init__(self, x, y=\"blublu\"):\n","        self.x = x\n","        self.y = y\n","\n","    def run(self):\n","        return self.x / 10"]},{"cell_type":"markdown","metadata":{"id":"5YbM6UNWStx6"},"source":["This is not great, because `self.x` needs to be an number but nothing is checking if the correct type of x has been passed in the attribute.\n","\n","So a better version would be"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MM35P0MWStx6"},"outputs":[],"source":["class MyTask:\n","    def __init__(self, x, y=\"blublu\"):\n","        # Type checks\n","        if not isinstance(x, int):\n","            print(f\"checking type of x: {type(x)}\")\n","            raise TypeError(f\"x must be an int, got {type(x)}\")\n","        if not isinstance(y, str):\n","            print(f\"checking type of y: {type(y)}\")\n","            raise TypeError(f\"y must be a str, got {type(y)}\")\n","\n","        self.x = x\n","        self.y = y\n","\n","    def run(self):\n","        return self.x / 10\n","\n","mytask = MyTask(x=\"blublu\")  # This will raise a TypeError because x is not an int\n","\n","print(mytask.run())"]},{"cell_type":"markdown","metadata":{"id":"By9owjPPStx6"},"source":["With increasing number of attributes and complex classes this might get very complicated.\n","\n","This is where Pydantic comes in — it automatically creates the `__init__` method for you, handles default values, and checks that every value is the right type. Instead of writing a lot of boilerplate code, you just describe your data once using type hints, and Pydantic takes care of the rest.\n","\n","A simple example like this...\n","\n","Try uncommenting second line of class instantiation to see the Validation error"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BtnOsORpStx6"},"outputs":[],"source":["class MyTask(pydantic.BaseModel):\n","    model_config = pydantic.ConfigDict(extra=\"forbid\")  # pydantic boilerplate\n","    x: int\n","    y: str = \"blublu\"\n","\n","    def run(self):\n","        return self.x / 10\n","\n","mytask = MyTask(x=12)\n","mytask.x  # this is 12\n","\n","MyTask(x=\"blublu\")  # uncomment this line and rerun to see the validation error\n","# >> ValidationError: 1 validation error for MyTask (x hould be a valid integer)"]},{"cell_type":"markdown","metadata":{"id":"cncZQ8jMStx6"},"source":["The syntax for type hinting here is `attr_name: type` and if you wish to set a default value, follow by ` = ...`. Let's go back to our example `class Feature`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"18ZgQBO-Stx6"},"outputs":[],"source":["# class Neuro(pydantic.BaseModel):\n","\n","#     preproc_path: Path  # no default value\n","#     fmin: float = 0.05  # default value\n","#     fmax: float = 40.\n","#     ...\n","\n","# Feature class that computes word frequency or word embedding\n","# class Feature(pydantic.BaseModel):\n","#     feature: tp.Literal['zipf_frequency', 'word_embedding']\n","#     ...\n","\n","# tp.Literal lets you specify that a variable can only take one or more specific constant values"]},{"cell_type":"markdown","metadata":{"id":"8nUZTNqTStx7"},"source":["This makes sure that all the attributes of the class are set correctly before you actually start running everything.\n","\n","Pydantic also supports hierarchical class instantiation. So instead of having to instantiate `Data` before initializing `Experiment` we can pass an hierarchical config to instantiate `Experiment` in one line."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rel_nWVhStx7"},"outputs":[],"source":["# Experiment class that runs the decoding experiment\n","# class Experiment(pydantic.BaseModel):\n","#     data: Data  # here after instantiation, self.data will be an instance of class Data\n","#     n_folds: int = 5\n","#     ..."]},{"cell_type":"markdown","metadata":{"id":"m38VlEnPStx7"},"source":["## Caching with Exca\n","\n","Caching is a way of storing the result of something so you don’t have to do the same work again the next time you need it.\n","\n","Exca provides a easy interface to achieve this, makes it easy to work with local and remote computations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hFaHBqQpStx7"},"outputs":[],"source":["# example task\n","class TutorialTask(pydantic.BaseModel):\n","    param: int = 12\n","\n","    def process(self) -> float:\n","        return self.param * np.random.rand()"]},{"cell_type":"markdown","metadata":{"id":"WManp7wpStx7"},"source":["We add exca to cache the output of this process\n","\n","`@func` is decorator, which is a function that wraps another function to modify or extend its behavior without changing its code."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXqFgyF2Stx7"},"outputs":[],"source":["class TutorialTask(pydantic.BaseModel):\n","    param: int = 12\n","    infra: TaskInfra = TaskInfra()\n","\n","    @infra.apply\n","    def process(self) -> float:\n","        return self.param * np.random.rand()"]},{"cell_type":"markdown","metadata":{"id":"uo3_LaiBStx7"},"source":["TaskInfra provides configuration for caching and computation, in particular providing a folder activates caching through the filesystem. Here we use `CACHE_DIR`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHPtuZuwStx7"},"outputs":[],"source":["task = TutorialTask(param=1, infra={\"folder\": CACHE_DIR})\n","out = task.process()\n","print(out)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dZ_eRe3JStx7"},"outputs":[],"source":["# calling process again will load the cache and not a new random number\n","assert out == task.process()"]},{"cell_type":"markdown","metadata":{"id":"JcDW9OujStx7"},"source":["Let's have a look at what is in the cache directory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HL6aDP_aStx7"},"outputs":[],"source":["# inspect content of cache dir\n","for dir in Path(CACHE_DIR).iterdir():\n","    print(dir.name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_vqpJMqStx7"},"outputs":[],"source":["tutorial_task_cache = Path(CACHE_DIR) / \"__main__.TutorialTask.process,0\" / \"param=1-2c59e2aa\"\n","# let's have a look at the content of the cache\n","for file_path in tutorial_task_cache.iterdir():\n","    print(file_path.name)"]},{"cell_type":"markdown","metadata":{"id":"Gak0n-rVStx7"},"source":["Now let's try adding the TaskInfra to this class that does some simple math computations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cM_jFTGUStx7"},"outputs":[],"source":["# try it yourself!\n","class MathTask(pydantic.BaseModel):\n","    a: float = 1.0\n","    b: float = 1.0\n","    operation: tp.Literal[\"sum\", \"diff\", \"prod\", \"div\"] = \"sum\"\n","    ...\n","\n","    ...\n","    def process(self) -> float | dict:\n","        results = {\n","            \"sum\": self.a + self.b,\n","            \"diff\": self.a - self.b,\n","            \"prod\": self.a * self.b,\n","            \"div\": self.a / self.b if self.b != 0 else float('inf')\n","        }\n","        if self.operation == \"all\":\n","            return results\n","        return results[self.operation]\n","\n","config = f\"\"\"\n","a: 1.0\n","b: 2.0\n","operation: sum\n","infra:\n","    folder: {CACHE_DIR}\n","\"\"\"\n","\n","config = yaml.safe_load(config)\n","task = MathTask(**config)\n","out = task.process()\n","print(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oNVgD-EYStx7"},"outputs":[],"source":["# check cache dir that indeed the results are cached\n","for dir in Path(CACHE_DIR).iterdir():\n","    print(dir.name)"]},{"cell_type":"markdown","metadata":{"id":"rkVLrKrsStx7"},"source":["# Coming back to the whole pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iLK9hw7EStx7"},"outputs":[],"source":["# Neuro class that handles the neural data\n","class Neuro(pydantic.BaseModel):\n","\n","    preproc_path: Path\n","    fmin: float = 0.05\n","    fmax: float = 40.\n","    freq: float = 80.\n","    tmin: float = -.5\n","    tmax: float = 1.\n","\n","    model_config = pydantic.ConfigDict(extra=\"forbid\")  # forbit model to accept extra attributes\n","\n","    def prepare_neuro(self, session: str) -> mne.io.Raw:\n","\n","        \"\"\" Load the raw neuro data and filter \"\"\"\n","\n","        file= Path(self.preproc_path) / f\"{session}_preproc.fif\"\n","\n","        if file.exists():\n","            raw = mne.io.read_raw(file, verbose=\"ERROR\")\n","\n","        else:\n","            fmin = self.fmin\n","            fmax = self.fmax\n","            freq = self.freq\n","\n","            original_file = self.preproc_path / f\"{session}.fif\" # original file\n","            raw = mne.io.read_raw(original_file)\n","            raw = raw.pick(picks=[\"meg\"]) # don't want to analyse misc\n","\n","            # band pass filter\n","            raw = raw.filter(fmin, fmax)\n","\n","            # downsample\n","            if freq != raw.info[\"sfreq\"]:\n","                raw = raw.resample(freq)\n","\n","        return raw\n","\n","    def __call__(self, session:str) -> tuple[np.ndarray, list[str]]:\n","\n","        \"\"\" Segment the neural data around words \"\"\"\n","\n","        raw = self.prepare_neuro(session)\n","\n","        events = pd.read_csv(self.preproc_path / \"events.csv\")\n","\n","        # Select the words in the relevant session\n","        words = events[(events['type'] == 'Word') & (events['session'] == session)].dropna().reset_index(drop=True)\n","\n","        # Get word onsets in samples\n","        word_onsets = np.ones((len(words), 3), dtype=int) # mne.epochs expects events of shape (n_events, 3) but we are only interested in the first column here -> set the rest to 0\n","        word_onsets[:, 0] = words.start *raw.info[\"sfreq\"] # first column must contain the onset of each event (word) in samples\n","\n","        # Segment\n","        segments = mne.Epochs(\n","            raw,\n","            word_onsets,\n","            metadata=words,\n","            event_repeated=\"drop\",\n","            baseline=(-0.2, 0),  # setting a baseline (-0.2, 0) can improve decoding results. Baselining subtracts the mean value over this window from the entire segment.\n","            tmin=self.tmin,\n","            tmax=self.tmax,\n","            verbose=\"ERROR\"\n","        )\n","\n","        # from mne to numpy\n","        neuro_array = segments.get_data(verbose=\"ERROR\")\n","\n","        # clip segments to prevent outliers impacting regression\n","        neuro_array = np.clip(neuro_array, a_min=-20, a_max=20)\n","\n","        n_words, n_channels, n_times = neuro_array.shape\n","        words = segments.metadata['text']\n","\n","        return neuro_array, words\n","\n","# Feature class that computes word frequency or word embedding\n","class Feature(pydantic.BaseModel):\n","    feature: tp.Literal['zipf_frequency', 'word_embedding']\n","    _model = None\n","\n","    model_config = pydantic.ConfigDict(extra=\"forbid\")  # forbit model to accept extra attributes\n","\n","    def __call__(self, word: str) -> list[float]:\n","\n","        \" Compute word frequency or word embedding for a single wordi \"\n","\n","        if self.feature == 'zipf_frequency':\n","            return [zipf_frequency(word, 'en')]\n","        else:\n","            if self._model is None:\n","                self._model = spacy.load(\"en_core_web_lg\")\n","            word_embedding = self._model(word).vector\n","            return word_embedding.tolist()\n","\n","# Data class that combines neural data and feature extraction\n","class Data(pydantic.BaseModel):\n","    neuro: Neuro\n","    feature: Feature\n","    n_sessions: int = 1\n","\n","    model_config = pydantic.ConfigDict(extra=\"forbid\")  # forbit model to accept extra attributes\n","\n","    def __call__(self) -> tuple[np.ndarray, np.ndarray]:\n","\n","        \"\"\" concatenate neural data and feature over multiple sessions \"\"\"\n","\n","        print(\"Preparing data\")\n","\n","        # get data\n","        neuro_array = []\n","        words = []\n","\n","        sessions = [i.name.split(\"_preproc\")[0] for i in sorted(list(self.neuro.preproc_path.glob(\"*.fif\")))] # neuro files available in path\n","        if self.n_sessions > len(sessions):\n","            raise ValueError(f\"You requested {self.n_sessions} but there are only {len(sessions)} available.\")\n","\n","        for session in sessions[:self.n_sessions]:\n","            session_neuro_array, session_words = self.neuro(session)\n","            neuro_array.append(session_neuro_array)\n","            words.extend(session_words)\n","        neuro_array = np.concatenate(neuro_array, 0)\n","\n","        # compute embedding\n","        feature_array = np.asarray([self.feature(word) for word in words])\n","        n_words, n_dims = feature_array.shape\n","        return neuro_array, feature_array\n","\n","# Experiment class that runs the decoding experiment\n","class Experiment(pydantic.BaseModel):\n","    data: Data\n","    n_folds: int = 5\n","    infra: TaskInfra = TaskInfra() # infra function for caching and and remote computation\n","\n","    model_config = pydantic.ConfigDict(extra=\"forbid\")  # forbit model to accept extra attributes\n","\n","    @infra.apply\n","    def run(self) -> np.ndarray:\n","\n","        \"\"\" fit a linear decoding model \"\"\"\n","\n","        print(f\"Decoding {self.data.feature.feature} from {self.data.n_sessions} sessions\")\n","\n","        neuro_array, feature_array = self.data()\n","        n_words, n_channels, n_times = neuro_array.shape\n","        n_words, n_dims = feature_array.shape\n","\n","        print(\"Fitting model\")\n","        # init model\n","        model = make_pipeline(StandardScaler(), Ridge(alpha=1e4))\n","\n","        # init cross validation\n","        cv = KFold(self.n_folds, shuffle=False)\n","\n","        # experiment loop\n","        r_scores = np.zeros((cv.n_splits, n_times, n_dims))\n","        for split, (train, test) in enumerate(cv.split(neuro_array)):\n","            for t in trange(n_times, desc=f\"split {split}/{cv.n_splits}\"):\n","                model.fit(X=neuro_array[train, :, t], y=feature_array[train])\n","                preds = model.predict(X=neuro_array[test, :, t])\n","\n","                for d in range(n_dims):\n","                    r_scores[split, t, d] = pearsonr(\n","                        preds[:, d],\n","                        feature_array[test, d]\n","                    ).statistic\n","        # mean across splits\n","        r = r_scores.mean(0)\n","        # mean across feature dimensions\n","        r = r.mean(1)\n","        return r\n"]},{"cell_type":"markdown","metadata":{"id":"T0v31exIStx8"},"source":["# Now instead of decoding zipfreq, let's try decoding word embeddings\n","\n","Modify the following config to run linear decoding on Word Embedding features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BPIgiwE3Stx8"},"outputs":[],"source":["config = f\"\"\"\n","data:\n","    neuro:\n","        preproc_path: {DATA_DIR}\n","        fmin: 0.05\n","        fmax: 40.\n","        freq: 80.\n","    feature:\n","        feature: ...\n","    n_sessions: 1\n","n_folds: ..\n","infra:\n","    folder: {CACHE_DIR}\n","\"\"\"\n","\n","config = yaml.safe_load(config)\n","exp = Experiment(**config)\n","\n","# Run the experiment\n","r = exp.run()\n","\n","times = np.linspace(exp.data.neuro.tmin, exp.data.neuro.tmax, r.shape[0])\n","plot_results(times, r, feature_name=exp.data.feature.feature)\n"]},{"cell_type":"markdown","metadata":{"id":"dSuIVvDqStx8"},"source":["# Conclusion\n","## Why Exca\n","\n","- built on top of pydantic: a package providing model/configuration classes and allows for parameter validation when instantiating the object\n","- provides “infra” pydantic configuration that can be part of a parent pydantic configuration and change the way it behaves\n","- let's one add caching and remote computation to its methods\n","\n","**More on the philosophy of exca**: https://facebookresearch.github.io/exca/infra/explanation.html#philosophy"]},{"cell_type":"markdown","metadata":{"id":"W1_7WdprStx8"},"source":["# Bonus"]},{"cell_type":"markdown","metadata":{"id":"9G1Uw7h7Stx9"},"source":["## Working with clusters"]},{"cell_type":"markdown","metadata":{"id":"3j-BToLZStx9"},"source":["##### Local/remote submission with exca\n","- use exca.Taskinfra to configure when computation is run\n","- in remote case, parameter validation happens locally before the jobs are sent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7rrLtNpStx9"},"outputs":[],"source":["class MyTask(pydantic.BaseModel): # local task\n","    x: int\n","    y: str = \"blublu\"\n","    infra: TaskInfra = TaskInfra()\n","    model_config = pydantic.ConfigDict(extra=\"forbid\")  # forbit model to accept extra attributes\n","\n","    @infra.apply\n","    def compute(self) -> float:\n","        return self.x / 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02ikpSyfStx9"},"outputs":[],"source":["# remote task example\n","config = f\"\"\"\n","x: 12\n","y: whatever\n","infra:  # resource parameters\n","  cluster: local\n","  folder: {CACHE_DIR}\n","  cpus_per_task: 4\n","\"\"\"\n","# infra.cluster set to slurm when using slurm\n","\n","dictconfig = yaml.safe_load(config)\n","obj = MyTask(**dictconfig)  # validation happens locally\n","out = obj.compute()  # runs in a slurm job!\n","assert out == 1.2"]},{"cell_type":"markdown","metadata":{"id":"r_5h4PlTStx9"},"source":["So the following situation wouldn't happen..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MNvZXZd1Stx9","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["srun --cpus-per-task=4 --time=60 python -m mytask --z=12\n","\n",">> srun: job 34633429 queued and waiting for resources\n",">> srun: job 34633429 has been allocated resources\n","...\n","...\n",">> usage: mytask.py [-h] [--x X] [--y Y]\n",">> mytask.py: error: unrecognized arguments: --z=12\n",">> srun: error: learnfair0478: task 0: Exited with exit code 2"]},{"cell_type":"markdown","metadata":{"id":"kHd9VqAdStx9"},"source":["## Advance functionality: Discriminated Union"]},{"cell_type":"markdown","metadata":{"id":"HdnHvv51Stx9","vscode":{"languageId":"shellscript"}},"source":["A discriminated union is a type that can hold one of several different, but fixed, types, each identified by a unique literal property (the “discriminator”). This allows safe type narrowing based on that property, letting the program know exactly which variant it’s working with."]},{"cell_type":"markdown","metadata":{"id":"wkhE3HDbStx9","vscode":{"languageId":"shellscript"}},"source":["#### Challenge: Complex experiments & Modularity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCpH7eTQStx9"},"outputs":[],"source":["import pydantic\n","import torch\n","\n","class Meg:\n","    def __init__(self, meg_type: str):\n","        self.meg_type = meg_type\n","\n","class Eeg:\n","    def __init__(self, fmri_type: str):\n","        self.eeg_type = fmri_type\n","\n","class Neuro:\n","    def __init__(self, neuro_type: str, meg_type: str = None, eeg_type: str = None):\n","        if neuro_type == \"meg\" :\n","            self.meg = Meg(meg_type)\n","        elif neuro_type == \"eeg\":\n","            self.fmri = Eeg(eeg_type)\n","        ...\n","\n","# then in your code\n","data = Neuro(neuro_type=\"meg\", meg_type=\"mag\", eeg_type=None)"]},{"cell_type":"markdown","metadata":{"id":"WQXkuBFhStx9"},"source":["But here in your `Neuro` you will have an additional attr `eeg_type` even though you're using `Meg`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VaZlRJIoStx9"},"outputs":[],"source":["# use discriminated unions here, emphasize modularity\n","from pydantic import BaseModel\n","import typing as tp\n","\n","class Meg(BaseModel):\n","    name: tp.Literal[\"Meg\"] = \"Meg\"\n","    meg_type: str = \"mag\"\n","    model_config = pydantic.ConfigDict(extra=\"forbid\")  # pydantic boilerplate: safer\n","\n","\n","class Eeg(BaseModel):\n","    name: tp.Literal[\"Eeg\"] = \"Eeg\"\n","    eeg_type: str = \"eeg\"\n","    model_config = pydantic.ConfigDict(extra=\"forbid\")\n","\n","class Neuro(pydantic.BaseModel):\n","    device: Meg | Eeg = pydantic.Field(..., discriminator=\"name\")  # | indicates a union\n","    n_recordings: int\n","    ...\n","\n","config = {\n","    \"device\": {\n","        \"name\": \"Meg\",\n","        \"meg_type\": \"mag\"\n","    },\n","    \"n_recordings\": 3,\n","}\n","\n","neuro = Neuro(**config)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKRXTCtIStx9"},"outputs":[],"source":["config = {\n","    \"device\": {\n","        \"name\": \"Meg\",\n","        \"eeg_type\": \"mag\"\n","    },\n","    \"n_recordings\": 3,\n","}\n","\n","neuro = Neuro(**config)\n"]},{"cell_type":"markdown","metadata":{"id":"W7K-K6-3oZ9A"},"source":["## MapInfra\n","The TaskInfra above is for methods that do not take additional arguments / computations that are fully defined by the configuration such as an experiment/a training for instance. Consider now that the configuration defines a computation to be applied to a list of items (eg: process a list of images / texts etc), this is the use case for the MapInfra:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JsJp3LiEStx9"},"outputs":[],"source":["import typing as tp\n","import pydantic\n","import numpy as np\n","from exca import MapInfra\n","\n","class TutorialMap(pydantic.BaseModel):\n","    param: int = 12\n","    infra: MapInfra = MapInfra(version=\"1\")\n","\n","    @infra.apply(item_uid=str)\n","    def process(self, items: tp.Iterable[int]) -> tp.Iterator[np.ndarray]:\n","        for item in items:\n","            yield np.random.rand(item, self.param)"]},{"cell_type":"markdown","metadata":{"id":"TyRP_dFdo4m7"},"source":["As opposed to the TaskInfra, the MapInfra.apply method now requires an item_uid parameter that states how to map each item of the input iterable into a unique string which will be used for identification/caching.\n","\n","From then, calling whatever.process([1, 2, 3]) will trigger (possibly) remote computation and caching/storage. You can control the remote resources through the infra instance. Eg: the following will trigger the computation in the current process (change \"cluster\": None to auto to have it run on slurm cluster if available or in a dedicated process)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JShQFe12of3k"},"outputs":[],"source":["mapper = TutorialMap(infra={\"cluster\": None, \"folder\": CACHE_DIR, \"cpus_per_task\": 1})\n","mapper.process([1, 2, 3])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SZ6HSrjIo_Hd"},"outputs":[],"source":["import spacy\n","import time\n","\n","class SpacyEmbedding(BaseModel):\n","    \"\"\"\n","    Spacy embedding class for text data.\n","    \"\"\"\n","    name: tp.Literal[\"SpacyEmbedding\"] = \"SpacyEmbedding\"\n","    model_name : str = \"en_core_web_lg\"\n","\n","    infra: MapInfra = MapInfra(version=\"v1\")\n","    model_config = pydantic.ConfigDict(extra=\"forbid\")\n","\n","    @infra.apply(item_uid=lambda x: str(x))\n","    def embed(self, words: list[str]) -> tp.Iterator[np.array]:\n","        \"\"\"\n","        Generate embeddings for a list of words.\n","        \"\"\"\n","        import spacy.cli\n","        spacy.cli.download(self.model_name)\n","        model = spacy.load(self.model_name)\n","        for word in words:\n","            yield model(word).vector.astype(np.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rSiuaXaYpCOc"},"outputs":[],"source":["spacy_config = {\n","    \"model_name\": \"en_core_web_lg\",\n","    \"infra\": {\n","        \"cluster\": None,\n","        \"folder\": CACHE_DIR,\n","    }\n","}\n","spacy_embedder = SpacyEmbedding(**spacy_config)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}